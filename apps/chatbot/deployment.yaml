apiVersion: v1
kind: Namespace
metadata:
  name: chatbot
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: chatbot-code
  namespace: chatbot
data:
  app.py: |
    from fastapi import FastAPI
    from fastapi.responses import HTMLResponse, StreamingResponse
    from fastapi.middleware.cors import CORSMiddleware
    from pydantic import BaseModel
    import httpx
    import json
    import os

    app = FastAPI(title="Platform Assistant")
    app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

    OLLAMA_URL = os.getenv("OLLAMA_URL", "http://ollama.ollama.svc:11434")
    MODEL = os.getenv("OLLAMA_MODEL", "llama3.2")

    SYSTEM_PROMPT = """You are the AI assistant for Alphonzo Jones Jr's Platform Engineering Lab.
    
    IMPORTANT: You are running LOCALLY on a Tesla P4 GPU in Alphonzo's homelab - NOT in the cloud!
    
    Platform: 6-node Kubernetes cluster with GPU support
    GPU: NVIDIA Tesla P4 8GB on worker05 (HP Z240)
    AI Stack: Ollama + Open WebUI + This chatbot
    Services: 20+ including ArgoCD, Grafana, Vault, TaskApp, and more
    Backup: 3-2-1 strategy with Synology NAS and Backblaze B2
    
    About Alphonzo: Platform/DevOps Engineer, AWS certified (CCP + SAA), AAS in Cloud Computing from NOVA.
    
    Be helpful and concise. Mention you run on local GPU when relevant."""

    class ChatMessage(BaseModel):
        message: str

    class ChatResponse(BaseModel):
        response: str

    @app.get("/", response_class=HTMLResponse)
    async def home():
        return """<!DOCTYPE html>
    <html><head><title>Platform Assistant</title>
    <style>
    *{margin:0;padding:0;box-sizing:border-box}
    body{font-family:Arial,sans-serif;background:#0f0f23;color:#fff;min-height:100vh;display:flex;flex-direction:column}
    .header{background:rgba(255,255,255,.05);padding:20px;text-align:center}
    .header h1{color:#4ade80;font-size:1.8em}
    .badge{background:linear-gradient(135deg,#f59e0b,#ef4444);padding:4px 12px;border-radius:12px;font-size:12px}
    .chat-container{flex:1;max-width:800px;margin:0 auto;padding:20px;width:100%}
    .messages{padding:20px 0;overflow-y:auto;max-height:calc(100vh - 200px)}
    .message{margin-bottom:20px;display:flex;gap:12px}
    .message.user{flex-direction:row-reverse}
    .bubble{max-width:70%;padding:12px 16px;border-radius:16px;line-height:1.5}
    .message.assistant .bubble{background:rgba(255,255,255,.1)}
    .message.user .bubble{background:#4ade80;color:#000}
    .input-area{display:flex;gap:12px;padding:20px;background:rgba(255,255,255,.05);border-radius:16px}
    #messageInput{flex:1;background:rgba(255,255,255,.1);border:1px solid rgba(255,255,255,.2);border-radius:12px;padding:12px;color:#fff;font-size:16px}
    button{background:#4ade80;border:none;border-radius:12px;padding:12px 24px;color:#000;font-weight:600;cursor:pointer}
    button:disabled{opacity:.5;cursor:not-allowed}
    #messageInput:disabled{opacity:.5}
    .typing-indicator{display:flex;gap:4px;padding:4px 0}
    .typing-indicator span{width:8px;height:8px;background:rgba(255,255,255,.4);border-radius:50%;animation:bounce 1.4s infinite}
    .typing-indicator span:nth-child(2){animation-delay:.2s}
    .typing-indicator span:nth-child(3){animation-delay:.4s}
    @keyframes bounce{0%,80%,100%{transform:translateY(0)}40%{transform:translateY(-8px)}}
    </style></head>
    <body>
    <div class="header"><h1>Platform Assistant <span class="badge">Local AI</span></h1><p>Powered by Llama 3.2 on Tesla P4 GPU</p></div>
    <div class="chat-container">
    <div class="messages" id="messages">
    <div class="message assistant"><div class="bubble">Hi! I'm the Platform Assistant running on Alphonzo's Tesla P4 GPU. Ask me anything!</div></div>
    </div>
    <div class="input-area">
    <input type="text" id="messageInput" placeholder="Ask about the platform..." onkeypress="if(event.key==='Enter')sendMessage()">
    <button onclick="sendMessage()">Send</button>
    </div></div>
    <script>
    let streaming=false;
    async function sendMessage(){
      if(streaming)return;
      const input=document.getElementById("messageInput");
      const btn=document.querySelector("button");
      const msg=input.value.trim();if(!msg)return;
      input.value="";input.disabled=true;btn.disabled=true;streaming=true;
      addMsg(msg,"user");
      const msgs=document.getElementById("messages");
      const typingDiv=document.createElement("div");typingDiv.className="message assistant";typingDiv.id="typing";
      typingDiv.innerHTML='<div class="bubble"><div class="typing-indicator"><span></span><span></span><span></span></div></div>';
      msgs.appendChild(typingDiv);msgs.scrollTop=msgs.scrollHeight;
      try{
        const r=await fetch("/chat/stream",{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify({message:msg})});
        const reader=r.body.getReader();const dec=new TextDecoder();
        let bubble=null,buf="";
        while(true){
          const{done,value}=await reader.read();if(done)break;
          buf+=dec.decode(value,{stream:true});
          const lines=buf.split("\\n");buf=lines.pop();
          for(const line of lines){
            if(!line.startsWith("data: "))continue;
            const payload=line.slice(6);
            if(payload==="[DONE]")continue;
            try{
              const d=JSON.parse(payload);
              if(!bubble){const t=document.getElementById("typing");if(t)t.remove();const div=document.createElement("div");div.className="message assistant";bubble=document.createElement("div");bubble.className="bubble";div.appendChild(bubble);msgs.appendChild(div);}
              bubble.appendChild(document.createTextNode(d.token));
              msgs.scrollTop=msgs.scrollHeight;
            }catch(e){}
          }
        }
      }catch(e){const t=document.getElementById("typing");if(t)t.remove();addMsg("Connection error. Please try again.","assistant");}
      input.disabled=false;btn.disabled=false;streaming=false;input.focus();
    }
    function addMsg(t,type){
      const div=document.createElement("div");div.className="message "+type;
      const bubble=document.createElement("div");bubble.className="bubble";
      t.split("\\n").forEach((line,i)=>{if(i>0)bubble.appendChild(document.createElement("br"));bubble.appendChild(document.createTextNode(line));});
      div.appendChild(bubble);
      const msgs=document.getElementById("messages");msgs.appendChild(div);msgs.scrollTop=msgs.scrollHeight;
    }
    </script></body></html>"""

    @app.post("/chat", response_model=ChatResponse)
    async def chat(message: ChatMessage):
        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{OLLAMA_URL}/api/chat",
                    json={"model": MODEL, "messages": [{"role": "system", "content": SYSTEM_PROMPT}, {"role": "user", "content": message.message}], "stream": False},
                    timeout=180.0
                )
                if response.status_code == 200:
                    return ChatResponse(response=response.json()["message"]["content"])
                return ChatResponse(response=f"Error: {response.status_code}")
        except Exception as e:
            return ChatResponse(response=f"Error: {str(e)}")

    @app.post("/chat/stream")
    async def chat_stream(message: ChatMessage):
        async def generate():
            try:
                async with httpx.AsyncClient(timeout=180.0) as client:
                    async with client.stream("POST", f"{OLLAMA_URL}/api/chat",
                        json={"model": MODEL, "messages": [{"role": "system", "content": SYSTEM_PROMPT}, {"role": "user", "content": message.message}], "stream": True}) as response:
                        buf = ""
                        async for chunk in response.aiter_text():
                            buf += chunk
                            while "\n" in buf:
                                line, buf = buf.split("\n", 1)
                                line = line.strip()
                                if not line:
                                    continue
                                data = json.loads(line)
                                token = data.get("message", {}).get("content", "")
                                if token:
                                    yield f"data: {json.dumps({'token': token})}\n\n"
                                if data.get("done"):
                                    yield "data: [DONE]\n\n"
            except Exception as e:
                yield f"data: {json.dumps({'token': f'Error: {str(e)}'})}\n\n"
                yield "data: [DONE]\n\n"
        return StreamingResponse(generate(), media_type="text/event-stream", headers={"Cache-Control": "no-cache", "X-Accel-Buffering": "no"})

    @app.get("/health")
    async def health():
        return {"status": "healthy", "backend": "ollama", "model": MODEL}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: chatbot
  namespace: chatbot
spec:
  replicas: 4
  selector:
    matchLabels:
      app: chatbot
  template:
    metadata:
      labels:
        app: chatbot
    spec:
      initContainers:
      - name: install-deps
        image: python:3.11-slim
        command: ['sh', '-c', 'pip install --target=/app-deps fastapi uvicorn httpx pydantic']
        volumeMounts:
        - name: app-deps
          mountPath: /app-deps
      containers:
      - name: api
        image: python:3.11-slim
        command: ['sh', '-c', 'export PYTHONPATH=/app-deps:$PYTHONPATH && cd /app && python -m uvicorn app:app --host 0.0.0.0 --port 8000']
        ports:
        - containerPort: 8000
        env:
        - name: OLLAMA_URL
          value: "http://ollama.ollama.svc:11434"
        - name: OLLAMA_MODEL
          value: "llama3.2"
        volumeMounts:
        - name: app-code
          mountPath: /app
        - name: app-deps
          mountPath: /app-deps
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 15
          periodSeconds: 30
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "256Mi"
            cpu: "200m"
      volumes:
      - name: app-code
        configMap:
          name: chatbot-code
      - name: app-deps
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: chatbot
  namespace: chatbot
spec:
  selector:
    app: chatbot
  ports:
  - port: 80
    targetPort: 8000
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: chatbot
  namespace: chatbot
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - chat.alphonzojonesjr.com
    secretName: chatbot-tls
  rules:
  - host: chat.alphonzojonesjr.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: chatbot
            port:
              number: 80
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
  namespace: chatbot
spec:
  podSelector: {}
  policyTypes:
  - Ingress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-ingress-nginx
  namespace: chatbot
spec:
  podSelector:
    matchLabels:
      app: chatbot
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: ingress-nginx
    ports:
    - port: 8000
